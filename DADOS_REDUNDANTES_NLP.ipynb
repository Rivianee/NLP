{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWSBUTrB9hD5K3xzoEhx0K"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Certifique-se de ter instalado as seguintes dependências:\n",
        "- pandas\n",
        "- torch\n",
        "- transformers\n",
        "- pytorch-lightning"
      ],
      "metadata": {
        "id": "22UbG0MnRbaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introdução\n",
        "Este projeto consiste em um sistema para treinamento e avaliação de modelos de processamento de linguagem natural (PLN) utilizando diferentes arquiteturas de modelos de linguagem pré-treinados, como BERT, RoBERTa e Longformer. O sistema é projetado para realizar tarefas específicas de classificação ou regressão em dados de notas médicas, utilizando técnicas avançadas de pré-processamento, treinamento e avaliação de modelos.\n"
      ],
      "metadata": {
        "id": "izvv5GmWRdkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Argumentos do Script\n",
        "O script principal aceita os seguintes argumentos de linha de comando:\n",
        "\n",
        "--CAMINHO_COORTE: Caminho para o arquivo CSV contendo os dados da coorte a serem processados.\n",
        "\n",
        "--TIPO_MODELO: Tipo de modelo a ser utilizado para treinamento e avaliação. Opções disponíveis incluem bert, roberta e longformer.\n",
        "\n",
        "--max_epochs: Número máximo de épocas para treinamento do modelo (padrão: 10).\n",
        "\n",
        "--learning_rate: Taxa de aprendizado para o otimizador AdamW (padrão: 0.001).\n",
        "\n",
        "--batch_size: Tamanho do lote para o DataLoader durante o treinamento (padrão: 32)."
      ],
      "metadata": {
        "id": "i1mdah_FR3p5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F1fkM01Q8uo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import LongformerTokenizerFast, RobertaTokenizerFast, BertTokenizerFast, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "from deduper import Simple_deduper, dedup_note_df\n",
        "from utils import CATEGORIAS_NOTAS, _args4dedup, _task2target, _cohort2sets, _task2rawfile, _name4dedup\n",
        "\n",
        "\n",
        "class ConjuntoDedup(Dataset):\n",
        "    def __init__(self, args, stay_df, deduper, raw_text_df=None, deduped_data=None, tokenizer=None, token2id=None):\n",
        "        \"\"\"\n",
        "        Inicializa o conjunto de dados para modelagem com PyTorch.\n",
        "\n",
        "        Args:\n",
        "            args (Namespace): Argumentos do programa.\n",
        "            stay_df (pd.DataFrame): DataFrame contendo os dados da coorte.\n",
        "            deduper (Simple_deduper): Objeto deduplicador.\n",
        "            raw_text_df (pd.DataFrame, optional): DataFrame com texto bruto.\n",
        "            deduped_data (dict, optional): Dados deduplicados.\n",
        "            tokenizer (AutoTokenizer, optional): Tokenizer para modelos BERT.\n",
        "            token2id (dict, optional): Mapeamento de token para ID.\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.tarefa = args.TAREFA\n",
        "        self.deduper = deduper\n",
        "        self.tokenizer = tokenizer\n",
        "        self.token2id = token2id\n",
        "\n",
        "        self.dedup_args = _args4dedup(args)\n",
        "        self.skip_dedup = args.skip_dedup\n",
        "        self.skip_style = args.skip_style  # cabeça, cauda, cabeça-cauda se pular dedup\n",
        "        self.drop_type = args.drop_type  # padrão sem descarte ''\n",
        "\n",
        "        self.max_length = args.max_length\n",
        "        self.max_note_length = args.max_note_length\n",
        "        self.bert_max_length = args.bert_max_length\n",
        "\n",
        "        self.max_sent_num = args.max_sent_num\n",
        "        self.max_doc_num = args.max_doc_num\n",
        "\n",
        "        self.silent = args.silent\n",
        "        self.use_hierarch = 'hier' in args.TIPO_MODELO\n",
        "        self.use_sent_hier = args.TIPO_MODELO == 'hier3'\n",
        "\n",
        "        # carregar dados com sent mantido: (pt-doc-sent-token)\n",
        "        self.sent_kept = True\n",
        "\n",
        "        if raw_text_df is not None:\n",
        "            assert deduped_data is None, \"Não alimente duas fontes de dados juntas\"\n",
        "            self.data, self.text_lens = self.carregar_dados(stay_df, raw_text_df)\n",
        "        else:\n",
        "            assert raw_text_df is None, \"Não alimente duas fontes de dados juntas\"\n",
        "            self.data, self.text_lens = self.carregar_deduped_data(stay_df, deduped_data)\n",
        "\n",
        "    def carregar_dados(self, stay_df, raw_text_df):\n",
        "        \"\"\"\n",
        "        Carrega os dados brutos e deduplica conforme necessário.\n",
        "\n",
        "        Args:\n",
        "            stay_df (pd.DataFrame): DataFrame da coorte.\n",
        "            raw_text_df (pd.DataFrame): DataFrame com texto bruto.\n",
        "\n",
        "        Returns:\n",
        "            data: Dados carregados e deduplicados.\n",
        "            lens: Comprimentos dos dados.\n",
        "\n",
        "        \"\"\"\n",
        "        data, lens = [], []\n",
        "        for _, r in tqdm(stay_df.iterrows(), disable=self.silent, total=stay_df.shape[0]):\n",
        "            hadm = r['HADM_ID']\n",
        "            target = r[_task2target(self.tarefa)]\n",
        "\n",
        "            text_df = raw_text_df[raw_text_df.HADM_ID == hadm]\n",
        "            notas = dedup_note_df(text_df, CATEGORIAS_NOTAS, self.deduper, **self.dedup_args)\n",
        "            nota_parseada = self._parsear_notas(notas)\n",
        "\n",
        "            texto_codificado, comprimento = self._tokenizar_texto(nota_parseada)\n",
        "            data.append((texto_codificado, target, hadm))\n",
        "            lens.append(comprimento)\n",
        "\n",
        "        return data, lens\n",
        "\n",
        "    def carregar_deduped_data(self, stay_df, hadm2deduped):\n",
        "        \"\"\"\n",
        "        Carrega os dados deduplicados.\n",
        "\n",
        "        Args:\n",
        "            stay_df (pd.DataFrame): DataFrame da coorte.\n",
        "            hadm2deduped (dict): Dicionário de dados deduplicados por HADM_ID.\n",
        "\n",
        "        Returns:\n",
        "            data: Dados carregados e deduplicados.\n",
        "            lens: Comprimentos dos dados.\n",
        "\n",
        "        \"\"\"\n",
        "        data, lens = [], []\n",
        "        for _, r in tqdm(stay_df.iterrows(), disable=self.silent, total=stay_df.shape[0]):\n",
        "            hadm = r['HADM_ID']\n",
        "            target = r[_task2target(self.tarefa)]\n",
        "\n",
        "            notas = hadm2deduped[hadm]\n",
        "            nota_parseada = self._parsear_notas(notas)\n",
        "\n",
        "            texto_codificado, comprimento = self._tokenizar_texto(nota_parseada)\n",
        "            data.append((texto_codificado, target, hadm))\n",
        "            lens.append(comprimento)\n",
        "\n",
        "        return data, lens\n",
        "\n",
        "    def _tokenizar_texto(self, nota_parseada):\n",
        "        \"\"\"\n",
        "        Tokeniza o texto com base no tipo de modelo especificado.\n",
        "\n",
        "        Args:\n",
        "            nota_parseada (str or list): Nota ou lista de notas a serem tokenizadas.\n",
        "\n",
        "        Returns:\n",
        "            texto_codificado: Texto tokenizado.\n",
        "            comprimento: Comprimento do texto tokenizado.\n",
        "\n",
        "        \"\"\"\n",
        "        if self.tokenizer is not None:\n",
        "            if isinstance(nota_parseada, list):\n",
        "                nota_parseada = ' '.join(nota_parseada)\n",
        "            assert isinstance(nota_parseada, str)\n",
        "            texto_codificado = self.tokenizer(nota_parseada, max_length=self.bert_max_length, truncation=True, padding='max_length', return_token_type_ids=False)\n",
        "            comprimento = np.array(texto_codificado['attention_mask']).sum()\n",
        "\n",
        "        else:\n",
        "            if not self.use_hierarch:\n",
        "                notas_codificadas = [self._texto2id(nota, self.token2id) for nota in nota_parseada]\n",
        "                if self.max_note_length > 0:\n",
        "                    notas_codificadas = [nota[:self.max_note_length] for nota in notas_codificadas if len(nota) > 0]\n",
        "                texto_tokenizado = [i for j in notas_codificadas for i in j]\n",
        "                comprimento = len(texto_tokenizado[:self.max_length])\n",
        "\n",
        "                min_len = min(self.max_length, len(texto_tokenizado))\n",
        "                texto_codificado = np.zeros(self.max_length)\n",
        "                texto_codificado[:min_len] = texto_tokenizado[:min_len]\n",
        "\n",
        "            else:\n",
        "                if self.max_doc_num > 0:\n",
        "                    nota_parseada = nota_parseada[:self.max_doc_num]\n",
        "\n",
        "                if not self.sent_kept or not self.use_sent_hier:\n",
        "                    texto_codificado = [self._texto2id(nota, self.token2id) for nota in nota_parseada]\n",
        "                    texto_codificado = [texto[:self.max_note_length] for texto in texto_codificado if len(texto) > 0]\n",
        "                    comprimento = sum(len(t) for t in texto_codificado)\n",
        "                else:\n",
        "                    texto_codificado, comprimento = [], 0\n",
        "                    for doc in nota_parseada:\n",
        "                        doc_codificado = [self._texto2id(sent, self.token2id) for sent in doc]\n",
        "                        texto_codificado.append(doc_codificado[:self.max_sent_num])\n",
        "                        comprimento += sum(len(t) for t in doc_codificado)\n",
        "\n",
        "        return texto_codificado, comprimento\n",
        "\n",
        "    @staticmethod\n",
        "    def _texto2id(texto, token2id):\n",
        "        return [token2id[token.lower()] if token.lower() in token2id else token2id['<unk>'] for token in texto.split()]\n",
        "\n",
        "    def _parsear_notas(self, notas):\n",
        "        \"\"\"\n",
        "        Parseia as notas do paciente, filtrando categorias específicas se necessário.\n",
        "\n",
        "        Args:\n",
        "            notas (list): Lista de tuplas (categoria, texto).\n",
        "\n",
        "        Returns:\n",
        "            list: Lista de strings ou string única.\n",
        "\n",
        "        \"\"\"\n",
        "        if self.drop_type != '':\n",
        "            notas = self._drop_cat(notas, self.drop_type)\n",
        "\n",
        "        notas = [t for _, t in notas]\n",
        "\n",
        "        if self.use_hierarch:\n",
        "            if not self.use_sent_hier:\n",
        "                notas = [' '.join(sents) for sents in notas]\n",
        "            return notas\n",
        "        else:\n",
        "            if self.sent_kept:\n",
        "                notas = [' '.join(sents) for sents in notas]\n",
        "\n",
        "            if self.skip_dedup:\n",
        "                if self.skip_style == 'tail':\n",
        "                    return notas[::-1]\n",
        "                elif self.skip_style == 'headtail':\n",
        "                    return self._merge_select(notas, self.max_length)\n",
        "                else:\n",
        "                    return notas\n",
        "            else:\n",
        "                return notas\n",
        "\n",
        "    @staticmethod\n",
        "    def _drop_cat(lista_notas, to_drop, categorias=None):\n",
        "        \"\"\"\n",
        "        Remove categorias específicas das notas.\n",
        "\n",
        "        Args:\n",
        "            lista_notas (list): Lista de tuplas (categoria, nota).\n",
        "            to_drop (str): Categorias a serem removidas, separadas por '+'.\n",
        "\n",
        "        Returns:\n",
        "            list: Lista filtrada de tuplas (categoria, nota).\n",
        "\n",
        "        \"\"\"\n",
        "        if '+' in to_drop:\n",
        "            drop = to_drop.split('+')\n",
        "        else:\n",
        "            drop = [to_drop]\n",
        "\n",
        "        if categorias is None:\n",
        "            notas_drop = [n for n in drop if n in categorias]\n",
        "        else:\n",
        "            notas_drop = [n for n in drop if n in CATEGORIAS_NOTAS]\n",
        "\n",
        "        return [(cat, note) for cat, note in lista_notas if cat not in notas_drop]\n",
        "\n",
        "    def _merge_select(self, tlist, max_length):\n",
        "        \"\"\"\n",
        "        Combina as notas de cabeça e cauda.\n",
        "\n",
        "        Args:\n",
        "            tlist (list): Lista de notas.\n",
        "            max_length (int): Comprimento máximo da lista.\n",
        "\n",
        "        Returns:\n",
        "            list: Lista combinada de notas.\n",
        "\n",
        "        \"\"\"\n",
        "        outlist, count = [], 0\n",
        "        while count < max_length and count < len(tlist):\n",
        "            outlist.append(tlist[count])\n",
        "            count += 1\n",
        "            if count < max_length:\n",
        "                outlist.append(tlist[-count])\n",
        "                count += 1\n",
        "        return outlist\n",
        "\n",
        "\n",
        "class ModeloDedup(pl.LightningModule):\n",
        "    def __init__(self, args, model):\n",
        "        \"\"\"\n",
        "        Inicializa o modelo para modelagem com PyTorch Lightning.\n",
        "\n",
        "        Args:\n",
        "            args (Namespace): Argumentos do programa.\n",
        "            model: Modelo PyTorch.\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.model = model\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, targets = batch\n",
        "        outputs = self.model(inputs)\n",
        "        loss = self.criterion(outputs, targets)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs, targets = batch\n",
        "        outputs = self.model(inputs)\n",
        "        loss = self.criterion(outputs, targets)\n",
        "        return {'val_loss': loss, 'val_preds': outputs, 'val_targets': targets}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        val_preds = torch.cat([x['val_preds'] for x in outputs])\n",
        "        val_targets = torch.cat([x['val_targets'] for x in outputs])\n",
        "        val_acc = torch.sum(val_preds.argmax(dim=1) == val_targets).item() / len(val_targets)\n",
        "        self.log('val_loss', avg_loss, on_epoch=True)\n",
        "        self.log('val_acc', val_acc, on_epoch=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.args.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "def treinar_modelo(args, train_loader, val_loader):\n",
        "    \"\"\"\n",
        "    Função para treinar o modelo.\n",
        "\n",
        "    Args:\n",
        "        args (Namespace): Argumentos do programa.\n",
        "        train_loader (DataLoader): DataLoader para conjunto de treino.\n",
        "        val_loader (DataLoader): DataLoader para conjunto de validação.\n",
        "\n",
        "    Returns:\n",
        "        model: Modelo treinado.\n",
        "\n",
        "    \"\"\"\n",
        "    model = construir_modelo(args)\n",
        "    trainer = pl.Trainer(\n",
        "        gpus=args.gpus,\n",
        "        max_epochs=args.max_epochs,\n",
        "        progress_bar_refresh_rate=1,\n",
        "        weights_summary='full'\n",
        "    )\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    return model\n",
        "\n",
        "\n",
        "def construir_modelo(args):\n",
        "    \"\"\"\n",
        "    Constrói o modelo com base nos argumentos fornecidos.\n",
        "\n",
        "    Args:\n",
        "        args (Namespace): Argumentos do programa.\n",
        "\n",
        "    Returns:\n",
        "        model: Modelo construído.\n",
        "\n",
        "    \"\"\"\n",
        "    if args.TIPO_MODELO == 'bert':\n",
        "        tokenizer = BertTokenizerFast.from_pretrained(args.NOME_MODELO)\n",
        "        model = BertForSequenceClassification.from_pretrained(args.NOME_MODELO, num_labels=args.num_labels)\n",
        "    elif args.TIPO_MODELO == 'roberta':\n",
        "        tokenizer = RobertaTokenizerFast.from_pretrained(args.NOME_MODELO)\n",
        "        model = RobertaForSequenceClassification.from_pretrained(args.NOME_MODELO, num_labels=args.num_labels)\n",
        "    elif args.TIPO_MODELO == 'longformer':\n",
        "        tokenizer = LongformerTokenizerFast.from_pretrained(args.NOME_MODELO)\n",
        "        model = LongformerForSequenceClassification.from_pretrained(args.NOME_MODELO, num_labels=args.num_labels)\n",
        "    else:\n",
        "        raise ValueError(f\"Tipo de modelo {args.TIPO_MODELO} não suportado.\")\n",
        "\n",
        "    conjunto_treino = ConjuntoDedup(args, args.dados_treino, args.deduper, tokenizer=tokenizer)\n",
        "    conjunto_validacao = ConjuntoDedup(args, args.dados_validacao, args.deduper, tokenizer=tokenizer)\n",
        "\n",
        "    loader_treino = DataLoader(conjunto_treino, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
        "    loader_validacao = DataLoader(conjunto_validacao, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
        "\n",
        "    model = ModeloDedup(args, model)\n",
        "    return model\n",
        "\n",
        "\n",
        "def principal(args):\n",
        "    \"\"\"\n",
        "    Função principal para execução do código.\n",
        "\n",
        "    Args:\n",
        "        args (Namespace): Argumentos do programa.\n",
        "\n",
        "    \"\"\"\n",
        "    # Carregar dados\n",
        "    dados_texto_bruto = pd.read_csv(_task2rawfile(args))\n",
        "    coorte = pd.read_csv(args.CAMINHO_COORTE)\n",
        "\n",
        "    # Carregar modelo e iniciar treinamento\n",
        "    modelo = treinar_modelo(args, loader_treino, loader_validacao)\n",
        "    torch.save(modelo.state_dict(), args.CAMINHO_SALVAR_MODELO)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # Definir argumentos do parser aqui\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    principal(args)"
      ]
    }
  ]
}