# -*- coding: utf-8 -*-
"""ETAPA1_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iQhuoVy9fsum7UF-zd_7oWsYM-AwIwGi

## INSTALAÇÃO DE PACOTES E BIBLIOTECAS
"""

import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter
import spacy
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import numpy as np
import re
from nltk.corpus import stopwords
from nltk.stem import SnowballStemme
from sklearn.feature_extraction.text import TfidfVectorizer
import string
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
import unicodedata

# Download NLTK stopwords if needed (assuming Portuguese)
nltk.download('stopwords')
stop_words = set(stopwords.words('portuguese'))

import nltk
nltk.download('wordnet')

!pip install -U spacy
!python -m spacy download pt_core_news_sm

import nltk
nltk.download('punkt')
nltk.download('stopwords')

"""LEITURA EXCEL"""

import pandas as pd

# Substitua 'seu_arquivo.csv' pelo caminho para o seu arquivo CSV
caminho_arquivo = '/content/dataset.csv'

# Tente ler o arquivo CSV especificando a codificação correta
try:
    df = pd.read_csv(caminho_arquivo, encoding='latin1')

    # Corrigir as palavras erradas apenas nas colunas de texto
    for coluna in df.columns:
        if df[coluna].dtype == 'object':  # Verifica se a coluna contém texto
            df[coluna] = df[coluna].astype(str).apply(lambda x: x.encode('latin1').decode('utf-8', 'ignore'))

    # Exibir as primeiras linhas do DataFrame corrigido
    print(df.head())

except Exception as e:
    print("Ocorreu um erro ao tentar ler o arquivo CSV:", e)

"""# ANÁLISE EXPLORATÓRIA

"""

# Visualizar as primeiras linhas do DataFrame
print("Visualização das primeiras linhas do DataFrame:")
print(df.head())

# Resumo estatístico das colunas numéricas
print("\nResumo estatístico das colunas numéricas:")
print(df.describe())

# Verificar tipos de dados e informações do DataFrame
print("\nInformações do DataFrame:")
print(df.info())

# Verificar se há valores ausentes
print("\nValores ausentes por coluna:")
print(df.isnull().sum())

# Plotar a contagem de categorias
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='category')
plt.title('Contagem de Categorias')
plt.xlabel('Categoria')
plt.ylabel('Contagem')
plt.xticks(rotation=45)
plt.show()

# Plotar a contagem de subcategorias
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='subcategory')
plt.title('Contagem de Subcategorias')
plt.xlabel('Subcategoria')
plt.ylabel('Contagem')
plt.xticks(rotation=45)
plt.show()

# Distribuição do comprimento das sentenças
df['num_sentencas'] = df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))
plt.figure(figsize=(10, 6))
plt.hist(df['num_sentencas'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribuição do Comprimento das Sentenças')
plt.xlabel('Número de Sentenças')
plt.ylabel('Frequência')
plt.grid(True)
plt.show()

category_counts = df['category'].value_counts()
print(category_counts)

"""## PRÉ PROCESSAMENTO DOS DADOS"""

# Load spaCy Portuguese model (change 'pt_core_news_sm' for different models)
nlp = spacy.load("pt_core_news_sm")

# Function to combine pre-processing steps
def preprocess_text(text):
  """
  Pre-processes text data using NLTK and spaCy.

  Args:
      text (str): The text to be pre-processed.

  Returns:
      str: The pre-processed text.
  """

  # NLTK Pre-processing
  text = text.lower()  # Convert to lowercase
  text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation (using imported string module)
  tokens = text.split()  # Tokenize into words
  tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords (using downloaded stopwords)
  stemmer = SnowballStemmer('portuguese')  # Optional: Stemming (uncomment if needed)
  # tokens = [stemmer.stem(token) for token in tokens]  # Uncomment to apply stemming

  # spaCy Pre-processing (optional)
  doc = nlp(text)  # Create spaCy Doc object
  tokens = [token.text for token in doc if token.is_alpha]  # Extract alphanumeric tokens

  # Combine pre-processed tokens (consider customization for your task)
  preprocessed_text = ' '.join(tokens)
  return preprocessed_text

# Apply pre-processing to your DataFrame column
df['texto_preprocessado'] = df['text'].apply(preprocess_text)

# Print or save the pre-processed DataFrame
print(df[['text', 'texto_preprocessado']].head())  # Example: View pre-processing results
df.to_csv('preprocessed_data.csv', index=False)  # Example: Save pre-processed data

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove punctuation (using imported string module)
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Tokenize into words
    tokens = text.split()

    # Remove stopwords (using downloaded stopwords)
    stop_words = set(stopwords.words('portuguese'))
    tokens = [word for word in tokens if word not in stop_words]

    # Optional: Stemming (uncomment if needed)
    stemmer = SnowballStemmer('portuguese')
    # tokens = [stemmer.stem(token) for token in tokens]  # Uncomment to apply stemming

    # Combine pre-processed tokens (consider customization for your task)
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text

# Apply pre-processing to your DataFrame column
df['texto_preprocessado'] = df['text'].apply(preprocess_text)

# Print or save the pre-processed DataFrame
print(df[['text', 'texto_preprocessado']].head())  # Example: View pre-processing results
df.to_csv('preprocessed_data.csv', index=False)  # Example: Save pre-processed data

df = pd.read_csv('/content/preprocessed_data.csv')

def preprocess_text(text):
    """
    Função para pré-processar texto em português para classificação de notícias.

    Argumentos:
        text (str): O texto a ser pré-processado.

    Retorna:
        str: O texto pré-processado.
    """

    # Converter para minúsculas
    text = text.lower()

    # Remover pontuação (usando módulo string importado)
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Tokenizar em palavras
    tokens = text.split()

    # Remover stopwords (usando stopwords baixados)
    stop_words = set(stopwords.words('portuguese'))
    tokens = [word for word in tokens if word not in stop_words]

    # Normalização de acentos (opcional)
    normalized_tokens = []
    for token in tokens:
        normalized_token = unicodedata.normalize('NFKD', token).encode('ascii', 'ignore').decode('utf-8')
        normalized_tokens.append(normalized_token)

    # Lemmatization (opcional)
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = []
    for token in normalized_tokens:
        lemmatized_token = lemmatizer.lemmatize(token)
        lemmatized_tokens.append(lemmatized_token)

    # Optional: Stemming (descomente se necessário)
    stemmer = SnowballStemmer('portuguese')
    # tokens = [stemmer.stem(token) for token in tokens]  # Descomente para aplicar stemming

    # Combinar tokens pré-processados (considere personalização para sua tarefa)
    preprocessed_text = ' '.join(lemmatized_tokens)  # Use tokens lematizados para o texto final

    return preprocessed_text

# Aplicar pré-processamento à coluna do seu DataFrame
df['texto_preprocessado'] = df['text'].apply(preprocess_text)

# Imprimir ou salvar o DataFrame pré-processado
print(df[['text', 'texto_preprocessado']].head())  # Exemplo: Visualizar resultados do pré-processamento
df.to_csv('dados_pre_processados.csv', index=False)  # Exemplo: Salvar dados pré-processados
df.to_csv('preprocessed2_data.csv', index=False)  # Example: Save pre-processed data

"""## ANÁLISE EXPLORATÓRIA - DEPOIS DO TEXTO PROCESSADO e TESTES NO MODELO"""

import pandas as pd
df = pd.read_csv('/content/preprocessed2_data.csv')

import pandas as pd
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string

# Função para pré-processamento de texto
def preprocess_text(text):
    # Tokenização
    tokens = word_tokenize(text.lower())

    # Remoção de stopwords e pontuações
    stop_words = set(stopwords.words('portuguese') + list(string.punctuation))
    filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]

    return filtered_tokens

# Ler o arquivo CSV para um DataFrame
df = pd.read_csv("/content/preprocessed2_data.csv")

# Pré-processar os textos
df['text'] = df['text'].apply(preprocess_text)

# Calcular a frequência de palavras no geral
all_words = [word for text in df['text'] for word in text]
word_freq_all = Counter(all_words)

print("Frequência de palavras no geral:")
print(word_freq_all.most_common(10))  # Exibir as 10 palavras mais frequentes no geral

# Calcular a frequência de palavras por categoria
word_freq_by_category = {}
for category in df['category'].unique():
    category_words = [word for idx, row in df[df['category'] == category].iterrows() for word in row['text']]
    word_freq_by_category[category] = Counter(category_words)

# Exibir a frequência de palavras por categoria
for category, word_freq in word_freq_by_category.items():
    print(f"\nFrequência de palavras na categoria '{category}':")
    print(word_freq.most_common(10))  # Exibir as 10 palavras mais frequentes na categoria

# Definir as palavras a serem removidas
palavras_a_remover = {
    'mundo': ['sobre', 'anos'],
    'poder': ['sobre', 'afirmou', 'ainda'],
    'esporte': ['ainda'],
    'mercado': ['sobre', 'empresa'],
    'cotidiano': ['ainda'],
    'turismo': ['onde'],
    'ciencia': ['ainda', 'sobre', 'pode', 'ter'],
    'educacao': ['diz', 'segundo'],
    'comida': ['diz', 'onde']
    # Adicione outras categorias e palavras a serem removidas conforme necessário
}

# Carregar os dados pré-processados
df = pd.read_csv('/content/preprocessed2_data.csv')

# Função para remover palavras específicas de uma categoria
def remove_palavras_categoria(row):
    categoria = row['category']
    texto = row['text']

    # Verificar se a categoria tem palavras a remover definidas
    if categoria in palavras_a_remover:
        for palavra in palavras_a_remover[categoria]:
            texto = texto.replace(palavra, '')

    return texto

# Aplicar a remoção de palavras por categoria
df['text'] = df.apply(remove_palavras_categoria, axis=1)

# Salvar os dados modificados
df.to_csv('/content/preprocessed2_data_removido.csv', index=False)

# Carregar o DataFrame
df = pd.read_csv('/content/preprocessed2_data_removido.csv')



# Função para calcular a frequência de palavras por categoria
def word_frequency_by_category(dataframe, category_column, text_column):
    categories = dataframe[category_column].unique()
    word_freq_by_category = {}

    for category in categories:
        category_data = dataframe[dataframe[category_column] == category]
        all_words = []
        for text in category_data[text_column]:
            preprocessed_text = preprocess_text(text)
            all_words.extend(preprocessed_text)
        word_freq_by_category[category] = Counter(all_words)

    return word_freq_by_category

# Calcular a frequência de palavras por categoria
word_freq_by_category = word_frequency_by_category(df, 'category', 'text')

# Exibir as 10 palavras mais frequentes em cada categoria
for category, word_freq in word_freq_by_category.items():
    print(f"Frequência de palavras na categoria '{category}':")
    print(word_freq.most_common(10))
    print()

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
import pickle

# Definindo colunas de texto e rótulos
text_column = 'text'
label_column = 'category'

# Separando dados de treino e teste
X_treino, X_teste, y_treino, y_teste = train_test_split(
  df[text_column], df[label_column], test_size=0.2, random_state=42
)

# Codificando rótulos categóricos
label_encoder = LabelEncoder()
y_treino = label_encoder.fit_transform(y_treino)
y_teste = label_encoder.transform(y_teste)

# Definindo o modelo de Regressão Logística
modelo = LogisticRegression()

# Grades de hiperparâmetros
grades_parametros = {
  'C': [0.01, 0.1, 1],
  'max_iter': [1000, 2000, 5000],
}

# Criando pipelines para diferentes representações de texto
pipelines = []

# 1. TfidfVectorizer com normalização L2
pipeline_tfidf_l2 = Pipeline([
  ('tfidf', TfidfVectorizer(max_features=10000, norm="l2")),
  ('clf', GridSearchCV(modelo, grades_parametros, cv=5, scoring='accuracy')),
])
pipelines.append(('TfidfVectorizer L2', pipeline_tfidf_l2))

# Treinando e avaliando modelos
for nome_modelo, pipeline in pipelines:
  print(f"** Treinando e avaliando {nome_modelo} com GridSearchCV...")

# Treinando o pipeline
  pipeline.fit(X_treino, y_treino)

# Salvando o modelo treinado e o vectorizer
  with open(f'{nome_modelo}_model.pkl', 'wb') as model_file:
   pickle.dump(pipeline, model_file)

  with open(f'{nome_modelo}_vectorizer.pkl', 'wb') as vectorizer_file:
   pickle.dump(pipeline.named_steps['tfidf'], vectorizer_file)  # Use 'tfidf' here

# Previsão e avaliação no conjunto de teste
  y_pred = pipeline.predict(X_teste)
  print(f"Classification Report para {nome_modelo}:")
  print(classification_report(y_teste, y_pred, target_names=label_encoder.classes_))

# Salvando o codificador de rótulos
with open('label_encoder.pkl', 'wb') as encoder_file:
  pickle.dump(label_encoder, encoder_file)

from sklearn.feature_extraction.text import TfidfVectorizer

# Inicializando o vetorizador TF-IDF
vetorizador = TfidfVectorizer()

# Vetorizando os dados de treino e teste
X_treino_vetorizado = vetorizador.fit_transform(X_treino)
X_teste_vetorizado = vetorizador.transform(X_teste)

# Inicializando o classificador de regressão logística com o parâmetro class_weight
# 'balanced' atribui pesos inversamente proporcionais às frequências das classes
modelo = LogisticRegression(class_weight='balanced')

# Treinando o modelo
modelo.fit(X_treino_vetorizado, y_treino)

# Avaliando o modelo
acuracia = modelo.score(X_teste_vetorizado, y_teste)
print("Acurácia do modelo:", acuracia)

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
import pickle

# Definindo colunas de texto e rótulos
text_column = 'text'
label_column = 'category'

# Separando dados de treino e teste
X_treino, X_teste, y_treino, y_teste = train_test_split(
  df[text_column], df[label_column], test_size=0.2, random_state=42
)

# Codificando rótulos categóricos
label_encoder = LabelEncoder()
y_treino = label_encoder.fit_transform(y_treino)
y_teste = label_encoder.transform(y_teste)

# Definindo o modelo de Regressão Logística com balanceamento de classes
modelo = LogisticRegression(class_weight='balanced')

# Grades de hiperparâmetros
grades_parametros = {
  'C': [0.01, 0.1, 1],
  'max_iter': [1000, 2000, 5000],
}

# Criando pipelines para diferentes representações de texto
pipelines = []

# 1. TfidfVectorizer com normalização L2
pipeline_tfidf_l2 = Pipeline([
  ('tfidf', TfidfVectorizer(max_features=10000, norm="l2")),
  ('clf', GridSearchCV(modelo, grades_parametros, cv=5, scoring='accuracy')),
])
pipelines.append(('TfidfVectorizer L2', pipeline_tfidf_l2))

# Treinando e avaliando modelos
for nome_modelo, pipeline in pipelines:
  print(f"** Treinando e avaliando {nome_modelo} com GridSearchCV...")

# Treinando o pipeline
  pipeline.fit(X_treino, y_treino)

# Salvando o modelo treinado e o vectorizer
  with open(f'{nome_modelo}_model.pkl', 'wb') as model_file:
   pickle.dump(pipeline, model_file)

  with open(f'{nome_modelo}_vectorizer.pkl', 'wb') as vectorizer_file:
   pickle.dump(pipeline.named_steps['tfidf'], vectorizer_file)  # Use 'tfidf' here

# Previsão e avaliação no conjunto de teste
  y_pred = pipeline.predict(X_teste)
  print(f"Classification Report para {nome_modelo}:")
  print(classification_report(y_teste, y_pred, target_names=label_encoder.classes_))

# Salvando o codificador de rótulos
with open('label_encoder.pkl', 'wb') as encoder_file:
  pickle.dump(label_encoder, encoder_file)